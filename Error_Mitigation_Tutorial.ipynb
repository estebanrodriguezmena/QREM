{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error mitigation\n",
    "\n",
    "This notebook contains theoretical background and examples for using our Error Mitigation module with Qiskit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical background\n",
    "Here we describe main theoretical concepts related to mitigation procedure. For more detailed description, see Ref. [0].\n",
    "\n",
    "### Classical noise model\n",
    "Let us denote by $\\mathbf{M}$ a POVM describing noisy detector and $\\mathbf{P}$ denote ideal measurement. In classical noise model, we assume that the relation between the two is given by stochastic, invertible map $\\Lambda$:\n",
    "\n",
    "$$ \\mathbf{M} = \\Lambda \\mathbf{P}.$$\n",
    "\n",
    "Let us denote by $\\mathbf{p}_{exp}$ vector of probabilities obtained on a noisy detector $\\mathbf{M}$ measuring any quantum state, and by $\\mathbf{p_{ideal}}$ the analogous vector for ideal detector (for the same quantum state). From linearity of the Born's rule, it follows that two vectors are related by the same stochastic map as POVMs:\n",
    "\n",
    "$$ \\mathbf{p_{exp}} = \\Lambda \\mathbf{p_{ideal}}. $$\n",
    "\n",
    "Recall that we assumed that $\\Lambda$ is invertible. Hence by multiplying last equation by $\\Lambda^{-1}$ from both sides, we obtain\n",
    "\n",
    "$$ \\Lambda^{-1} \\mathbf{p_{exp}} = \\mathbf{p_{ideal}}. $$\n",
    "\n",
    "Effectively, by this kind of postprocessing, we obtain statistics which we would have obtained on the perfect detector devices.\n",
    "\n",
    "### Deviations from noise model\n",
    "#### Effects  on mitigation\n",
    "Above we assumed a very specific noise model. In practice it is likely that it will not be fulfilled exactly. In such a scenario, we may perform the following decomposition of POVM $M$ describing our device:\n",
    "$$ \\mathbf{M} = \\Lambda \\mathbf{P} + \\mathbf{\\Delta}\\ ,$$\n",
    "where $\\Delta$ denotes a \"coherent\" part of the noise and $\\Lambda$, as previously, is some stochastic, invertible map. \n",
    "\n",
    "In such a case, we can relate probability vector obtained on the noisy detector to ideal one in manner similar as before\n",
    "\n",
    "$$ \\mathbf{p_{exp}} = \\Lambda \\mathbf{p_{ideal}} + \\mathbf{d}\\ , $$\n",
    "where $\\mathbf{d}$ denotes a disturbance of probability vector due to existence of coherent part of the noise.\n",
    "If we now multiply the above expression by inverse of the noise matrix, we obtain\n",
    "\n",
    "$$ \\Lambda^{-1} \\mathbf{p_{exp}} = \\mathbf{p_{ideal}} + \\Lambda^{-1} \\mathbf{d}, \\  $$\n",
    "\n",
    "\n",
    "TODO: finish\n",
    "\n",
    "#### How to choose decomposition?\n",
    "In general, we can always perform decomposition into \"classical\" and \"coherent\" part in infinitely many ways. However, for the ideal detector $P$ modeled as projective measurement in computational basis, there exists a perfectly natural ansatz. Namely, we propose to consider diagonal parts of POVM's $\\mathbf{M}$ elements to describe classical part of the noise. \n",
    "As a justification for such choice, note that elements of $\\mathbf{P}$ for such ideal detector model are diagonal, and the stochastic map would preserve the diagonality. \n",
    "\n",
    "Hence, after obtaining description of POVM $\\mathbf{M}$ from detector tomography, reconstruction of $\\Lambda$ can be achieved by taking only diagonal parts of POVM's elements.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Finite-size statistics\n",
    "\n",
    "### When is mitigation succesfull?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mitigating the error using our module\n",
    "\n",
    "### Performing Quantum Detector Tomography\n",
    "Our error mitigation approach is based on the knowledge about the noise in the device's detector. Such knowledge can be obtained in procedure known as Quantum Detector Tomography (QDT). To perform QDT, one can follow the steps from our [QDT tutorial](https://github.com/fbm2718/QREM/blob/master/QDT_Tutorial.ipynb). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import povmtools\n",
    "import ancillary_functions as anf\n",
    "import numpy as np\n",
    "\n",
    "from qiskit import IBMQ, Aer, execute\n",
    "from qiskit.providers.aer import noise\n",
    "\n",
    "from quantum_tomography_qiskit import detector_tomography_circuits\n",
    "from DetectorTomographyFitter import DetectorTomographyFitter\n",
    "from QDTErrorMitigator import QDTErrorMitigator\n",
    "\n",
    "# Choose qubit indices\n",
    "QDT_qubit_index = [3]\n",
    "\n",
    "# Select probe kets\n",
    "QDT_probe_kets = povmtools.pauli_probe_eigenkets\n",
    "\n",
    "# Generate circuits\n",
    "QDT_circuits = detector_tomography_circuits(QDT_qubit_index, QDT_probe_kets)\n",
    "\n",
    "# Get QDT circuits results\n",
    "backend = Aer.get_backend('qasm_simulator')  #  Get backed\n",
    "shots_number = 2000  # Define number of measurement repetitions\n",
    "QDT_job = execute(QDT_circuits, backend=backend, shots=shots_number)\n",
    "results = QDT_job.result()\n",
    "\n",
    "# Get ml_povm_estimator using DTF and results\n",
    "DTF = DetectorTomographyFitter()\n",
    "ml_povm_estimator = DTF.get_maximum_likelihood_povm_estimator([results], QDT_probe_kets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing mitigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the estimator of POVM, we can create QDTErrorMitigator object and prepare it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation and preparation of QDTErrorMitigator\n",
    "mitigator = QDTErrorMitigator()\n",
    "mitigator.prepare_mitigator(ml_povm_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With prepared mitigator object we gain access to several useful functionalities. For example, we can:\n",
    "* Correct results of qiskit job by using apply_correction_to_qiskit_job(Results) method.\n",
    "* Access transition and correction matrices obtained from POVM given during preparation.\n",
    "\n",
    "In order to properly analyse the results of correction procedure, one have to be aware that in some cases raw application of $\\Lambda^{-1}$ to the results may yield quasiprobability (instead of probability) vectors. In such scenario our method calculates closest probability vectors and returns them instead. Distances from raw quasiprobabilities to returned probabilities, can be accessed via distances_from_closest_probability_vector member of mitigator object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error bounds\n",
    "\n",
    "With access to POVM and the correction and transition matrices, we are able to calculate bounds on several errors. In particular, using povtools module, we can calculate:\n",
    "* statistical error bound (using get_statistical_error_bound method),\n",
    "* coherent error bound (using gt_coherent_error_bound method),\n",
    "* correction error bound (using get_correction_error_bound_from_data or get_correction_error_bound_from_parameters method).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single qubit mitigation scenario\n",
    "\n",
    "In this section two examples of mitigation for single qubit will be shown. In order to show, that our mitigation scheme is efficient, we first need to create a noisy backend simulator. We start with required imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit, ClassicalRegister, QuantumRegister\n",
    "from qiskit import execute\n",
    "from qiskit import IBMQ, Aer\n",
    "from qiskit.providers.aer.noise import NoiseModel\n",
    "\n",
    "import povmtools\n",
    "from DetectorTomographyFitter import DetectorTomographyFitter\n",
    "from quantum_tomography_qiskit import detector_tomography_circuits\n",
    "from QDTErrorMitigator import QDTErrorMitigator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use standard qiskit methods to create noisy backend simulator. For this tutorial we will create simulator of IBM's Vigo device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  What I want to have first is working noisy backend simulation. In order to do that, I will use qiskit noise model\n",
    "# and download properties of selected backend.\n",
    "\n",
    "# Build noise model from backend properties.\n",
    "provider = IBMQ.load_account()\n",
    "backend = provider.get_backend('ibmq_vigo')\n",
    "noise_model = NoiseModel.from_backend(backend)\n",
    "\n",
    "# Get coupling map from backend, why not.\n",
    "coupling_map = backend.configuration().coupling_map\n",
    "\n",
    "# Get basis gates from noise model.\n",
    "basis_gates = noise_model.basis_gates\n",
    "\n",
    "# Finally, get the simulator backend.\n",
    "simulator_backend = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With backend created, what I need to do next is calculating POVM that describes its measurements best. To do that, following our QDT tutorial, I use DetectorTomographyFitter object from our module. In order to do that, I need to create calibration circuits and obtain their results first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 1987, '1': 13}\n",
      "{'0': 36, '1': 1964}\n",
      "{'0': 1038, '1': 962}\n",
      "{'0': 1043, '1': 957}\n",
      "{'0': 994, '1': 1006}\n",
      "{'0': 1006, '1': 994}\n"
     ]
    }
   ],
   "source": [
    "# What I want to do now, is prepare POVM for simulated backend. According to our other notebook, I prepare\n",
    "# circuits first.\n",
    "\n",
    "qdt_qubit_index = [0]\n",
    "qdt_probe_kets = povmtools.pauli_probe_eigenkets\n",
    "qdt_calibration_circuits = detector_tomography_circuits(qdt_qubit_index, qdt_probe_kets)\n",
    "\n",
    "# I then execute them on backend prepared earlier.\n",
    "shots_number = 2000\n",
    "\n",
    "# Perform a noisy simulation\n",
    "result = execute(qdt_calibration_circuits, simulator_backend, coupling_map=coupling_map, basis_gates=basis_gates,\n",
    "                 noise_model=noise_model, shots=shots_number)\\\n",
    "                 .result()\n",
    "\n",
    "# Print counts.\n",
    "for i in range(len(result.results)):\n",
    "    print(result.get_counts(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shot's number is obviously a matter of choice in this case. It's preferable to use more (real backends allows more that 8k shots per job) as in that way we obtain better statistics. With more accurate job results, we can -- obviously -- prepare better POVMs. Maximum likelihood POVM calculation is as easy as calling single method now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99369959+7.67339058e-19j -0.00125087+2.99909517e-03j]\n",
      " [-0.00125087-2.99909517e-03j  0.01858137-9.63724971e-19j]]\n",
      "[[0.00630041-7.67325823e-19j 0.00125087-2.99909517e-03j]\n",
      " [0.00125087+2.99909517e-03j 0.98141863-1.82198865e-16j]]\n"
     ]
    }
   ],
   "source": [
    "# With circuits results I can now use our Detector Tomography Fitter to obtain maximum likelihood POVM estimator.\n",
    "dtf = DetectorTomographyFitter()\n",
    "ml_povm_estimator = dtf.get_maximum_likelihood_povm_estimator([result], qdt_probe_kets)\n",
    "\n",
    "for m_i in ml_povm_estimator:\n",
    "    print(m_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With POVM calculated, we can now create and prepare mitigator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can use obtained POVM to correct to prepare mitigation object.\n",
    "mitigator = QDTErrorMitigator()\n",
    "mitigator.prepare_mitigator(ml_povm_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these preparations out of the way, we can now check how efficient our error mitigation approach is. Let's consider two simple scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X gate circuit\n",
    "\n",
    "In this case, we will first create a one qubit circuit (initially in |0><0| state) and we will apply X (not) operation to it. In ideal scenario we would expect all counts in state |1><1|. We begin with circuit creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<qiskit.circuit.instructionset.InstructionSet at 0x1539d7fa520>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to check how efficient our mitigator is, we need to obtain some noisy data first.\n",
    "# We will start with preparing simple experiment.\n",
    "\n",
    "qr = QuantumRegister(1, 'qreg')\n",
    "cr = ClassicalRegister(1, 'creg')\n",
    "qc = QuantumCircuit(qr, cr)\n",
    "\n",
    "qc.x(qr[0])\n",
    "qc.measure(qr, cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we execute this circuit on our simulator and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 33, '1': 1967}\n"
     ]
    }
   ],
   "source": [
    "result = execute(qc, simulator_backend, coupling_map=coupling_map, basis_gates=basis_gates,\n",
    "                 noise_model=noise_model, shots=shots_number)\\\n",
    "                 .result()\n",
    "\n",
    "for i in range(len(result.results)):\n",
    "    print(result.get_counts(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, noisy simulator returned some errors (|0><0| counts), as expected. Let's try to correct these results using our mitigator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good format\n",
      "[array([[0.],\n",
      "       [1.]])]\n"
     ]
    }
   ],
   "source": [
    "# Now let's correct them.\n",
    "corrected_results = mitigator.apply_correction_to_qiskit_job(result)\n",
    "print(corrected_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see here, is that instead of counts, our mitigator returned frequencies. In order to obtain counts we could multiply  corrected results times number of shots. This step isn't obviously necessary, as it can be clearly seen that results are indeed better, but let's do it for good measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 2000.0]\n"
     ]
    }
   ],
   "source": [
    "print([corrected_results[0][0][0] * shots_number, corrected_results[0][1][0] * shots_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we've got barely one count, out of 2000, in calculated in wrong state. Compared to raw 52 counts, this is a very good outcome. Let's try another test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H gate circuit\n",
    "\n",
    "In this case, we will use H (Hadamard) gate, instead of X. Everything else will be exactly like in the first scenario. We begin with creating and executing a circuit and then printing raw results of the job. We expect to obtain equal number of |1> and |0> states conuts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 1030, '1': 970}\n"
     ]
    }
   ],
   "source": [
    "# Let's try another example.\n",
    "qr2 = QuantumRegister(1, 'qreg2')\n",
    "cr2 = ClassicalRegister(1, 'creg2')\n",
    "qc2 = QuantumCircuit(qr2, cr2)\n",
    "\n",
    "qc2.h(qr2[0])\n",
    "qc2.measure(qr2, cr2)\n",
    "\n",
    "result = execute(qc2, simulator_backend, coupling_map=coupling_map, basis_gates=basis_gates,\n",
    "                 noise_model=noise_model, shots=shots_number)\\\n",
    "                 .result()\n",
    "\n",
    "for i in range(len(result.results)):\n",
    "    print(result.get_counts(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, that we again have got around 60 of wrong counts. Let's try to apply error mitigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good format\n",
      "[array([[0.50908558],\n",
      "       [0.49091442]])]\n"
     ]
    }
   ],
   "source": [
    "# Now let's correct them.\n",
    "corrected_results = mitigator.apply_correction_to_qiskit_job(result)\n",
    "print(corrected_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And print them as counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1018.1711624262215, 981.8288375737791]\n"
     ]
    }
   ],
   "source": [
    "print([corrected_results[0][0][0] * shots_number, corrected_results[0][1][0] * shots_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the results are better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is, however one important thing concerning __Hadamard circuit__ in particular. Due to probabilistic nature of noisy simulation it's possible to obtain perfect (or close to perfect) outcome. In such scenario it's possible for out mitigation procedure to worsen the results. This effect can be observed even in this tutorial, for discussed outcomes of the circuit.\n",
    "\n",
    "One can easily see, that determining if mitigation was a success or not is an important issue. This is especially true for experiments for which results are not known. Following procedure described in [0], by comparing operational distances and some error bounds, we can find out if mitigation failed or not! Let's do that. We start with calculating statistical error bound. We start with appointing mistake probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check out if mitigation was success. In order to do that, we have to calculate error bounds.\n",
    "# Let's assume (according to the notation form the main paper) that alpha=0.\n",
    "\n",
    "statistical_error_mistake_probability = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need counts, formatted as array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1030, 970]\n"
     ]
    }
   ],
   "source": [
    "# We need counts as array. The outcomes are 0 or 1, so we can create them in straightforward way.\n",
    "counts_dict = result.get_counts()\n",
    "counts = [counts_dict['0'], counts_dict['1']]\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use povmtools method to calculate statustical error bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.036394770800720934\n"
     ]
    }
   ],
   "source": [
    "# We calculate statistical error bound. Let's call it epsilon.\n",
    "epsilon = povmtools.get_statistical_error_bound(counts, statistical_error_mistake_probability)\n",
    "print(epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's, however, not enough. We'll also require correction error. Luckily, we have all the arguments necessary for its calculation using prepared povmtools method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04115515278501394\n"
     ]
    }
   ],
   "source": [
    "# We now need the correction error bound. Let's call it delta.\n",
    "delta = povmtools.get_correction_error_bound_from_data_and_statistical_error(ml_povm_estimator,\n",
    "                                                                             mitigator.correction_matrix, epsilon)\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last thing required for determining mitigation success is operational distance between perfect measurement and our ml_povm_estimator. We calculate it, once again, using povmtools module function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018998749666391156\n"
     ]
    }
   ],
   "source": [
    "# We also need operational distance, between perfect detector and our POVM. We calculate it.\n",
    "perfect_measurement = [[[1, 0], [0, 0]], [[0, 0], [0, 1]]]  # For one qubit!\n",
    "operational_distance = povmtools.operational_distance_POVMs(ml_povm_estimator, perfect_measurement)\n",
    "print(operational_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can check if our mitigation was a success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mitigation success: True\n"
     ]
    }
   ],
   "source": [
    "# Now we can check if mitigation can be deemed as success!\n",
    "print(f'Mitigation success: {delta < operational_distance + epsilon}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "[0] Filip B. Maciejewski, Zoltán Zimborás, Michał Oszmaniec, *Mitigation of readout noise in near-term quantum devices by classical post-processing based on detector tomography*, arxiv preprint, https://arxiv.org/abs/1907.08518 (2019)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
